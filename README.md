# SpiderWeb

![SpiderWeb Logo](https://github.com/medhatem/spiderweb/logo.png)

## Introduction

SpiderWeb is a web crawler designed to efficiently scrape and extract data from websites. It provides an interface for managing crawled data and running extraction processes.

## Features

- **Automated Web Crawling**: Easily scrape websites with configurable settings.
- **User Interface**: Manage crawled data through a web-based interface.
- **Flexible Configuration**: Adjust crawling parameters as needed.

## Demo

Watch the demonstration video to see SpiderWeb in action:

[![SpiderWeb Demo](https://img.youtube.com/vi/3W5Hb3GQodo/0.jpg)](https://youtu.be/3W5Hb3GQodo)

## Installation

### Prerequisites
- [Node.js](https://nodejs.org/) installed
- [Angular CLI](https://angular.io/cli) installed

### Setup Instructions
1. Clone the repository:
   ```bash
   git clone https://github.com/medhatem/spiderweb.git
   cd spiderweb
   ```
2. Install dependencies:
   ```bash
   npm install
   ```

## Usage

### Running the Crawler
To start the crawler, use the following command:
```bash
node crawler/server.js
```

### Running the Interface
To launch the web interface, use:
```bash
ng serve
```
Then, open a browser and navigate to `http://localhost:4200`.

## Contributing
We welcome contributions! To contribute:
1. Fork the repository.
2. Create a new feature branch:
   ```bash
   git checkout -b feature/your-feature-name
   ```
3. Make your changes and commit them:
   ```bash
   git commit -m "Add new feature"
   ```
4. Push to your branch:
   ```bash
   git push origin feature/your-feature-name
   ```
5. Open a pull request.

## License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

## Acknowledgements
Special thanks to all contributors and supporters of this project.

